from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM
from langchain_core.runnables import RunnableSequence

# Step 1: Define a reusable function that generates a prompt based on input variables
def generate_prompt(question_template: str, input_vars: dict) -> ChatPromptTemplate:
    """
    Generates a prompt by dynamically filling in the template with input variables.
    
    Parameters:
    question_template (str): The template string with placeholders.
    input_vars (dict): A dictionary of input variables to fill in the template.
    
    Returns:
    ChatPromptTemplate: A formatted prompt wrapped in a ChatPromptTemplate.
    """
    prompt_template = ChatPromptTemplate.from_template(template=question_template)
    return prompt_template

# Step 2: Define the first chain to get the name of the dish
def get_dish_name_chain(cuisine: str, type: str, meal: str, language:str, model_name: str):
    """
    Creates a chain that generates a dish name based on the meal.
    
    Parameters:
    cuisine (str): The selection of cuisine.
    type (str): The diet of the meal (vegetarian, vegan, meat-based)
    meal (str): The type of meal (e.g., "Breakfast", "Lunch").
    model_name (str): The name of the model to be used.
    
    Returns:
    str: The dish name generated by the model.
    """
    # Template to generate dish name
    question_template = "Suggest an easy-to-make {type} {cuisine} dish for {meal} in {language} language. Only provide the name of the dish and nothing else."
    input_vars = {"cuisine": cuisine,"type": type,"meal": meal, "language":language}
    
    # Generate the prompt dynamically
    prompt = generate_prompt(question_template, input_vars)
    
    # Initialize the model
    model = OllamaLLM(model=model_name)
    
    # Create a chain using RunnableSequence to connect the prompt to the model
    chain = RunnableSequence(prompt, model)
    
    # Invoke the chain to get the dish name
    dish_name = chain.invoke(input_vars)
    return dish_name.strip()  # Clean up the dish name

# Step 3: Define the second chain to get ingredients and recipe for the dish
def get_ingredients_and_recipe_chain(dish_name: str, language: str ,model_name: str):
    """
    Creates a chain that generates the ingredients and recipe for the given dish name.
    
    Parameters:
    dish_name (str): The name of the dish.
    model_name (str): The name of the model to be used.
    
    Returns:
    str: The ingredients and recipe generated by the model.
    """
    # Template to generate ingredients and recipe based on dish name
    question_template = "Provide the ingredients and recipe for {dish_name} and a brief nutritional info in {language} language."
    input_vars = {"dish_name": dish_name, "language": language}
    
    # Generate the prompt dynamically
    prompt = generate_prompt(question_template, input_vars)
    
    # Initialize the model
    model = OllamaLLM(model=model_name)
    
    # Create a chain using RunnableSequence to connect the prompt to the model
    chain = RunnableSequence(prompt, model)
    
    # Invoke the chain to get the ingredients and recipe
    recipe = chain.invoke(input_vars)
    return recipe.strip()

# Step 4: Define the sequential chain where the output of one is used as input to another
def sequential_chain(cuisine: str, type: str, meal: str,language: str, model_name: str):
    """
    Executes a sequential chain where the output of one chain (dish name) is used
    as the input for another chain (ingredients and recipe).
    
    Parameters:
    meal (str): The type of meal (e.g., "Breakfast", "Lunch").
    model_name (str): The name of the model to be used.
    
    Returns:
    tuple: The dish name, ingredients, and recipe generated by the chains.
    """
    # Step 1: Get the dish name
    dish_name = get_dish_name_chain(cuisine, type, meal, language ,model_name)
    #print(f"Dish Name: {dish_name}")
    
    # Step 2: Use the dish name to get ingredients and recipe
    recipe = get_ingredients_and_recipe_chain(dish_name,language, model_name)
    
    return dish_name, recipe

# if __name__ == "__main__":
#     cuisine = 'Italian'
#     diet = "Seafood"
#     meal = "Dinner"
#     model_name = "llama3.1"
#     language = "English"
#     dish_name, recipe = sequential_chain(cuisine,diet,meal, language, model_name)
#     print(f"Dish Name: {dish_name}")
#     print(f"Ingredients and Recipe: {recipe}")

